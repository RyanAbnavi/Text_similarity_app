{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text, RemoveStopWords=True):\n",
    "        self.text = text\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "        self.punctuations = ['.','!', '?', '\\'', '\\\"', ',', ':', ';', '(', ')', '[', ']', '<', '>', '\\n']\n",
    "        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "              'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "              'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "              'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "              'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "              'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "              'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "              'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "              'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "              'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',\n",
    "              'now', 'm', 're', 'would', 'd', 'll']\n",
    "        self.tokens = self.preprocess()\n",
    "    \n",
    "    def preprocess(self):\n",
    "        text = self.text.lower()\n",
    "        text = text.replace('-', '')\n",
    "        for p in self.punctuations:\n",
    "            text = text.replace(p, ' ')\n",
    "        \n",
    "        if self.RemoveStopWords:\n",
    "            tokens = [w for w in text.split(' ') if w not in self.stop_words and w!='']\n",
    "        else:\n",
    "            tokens = [w for w in text.split(' ') if w!='']\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, RemoveStopWords=True):\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "        self.vocabulary_ = {}\n",
    "        self.dictionary_ = {}\n",
    "        self.cv = []\n",
    "        self.tfidf = []\n",
    "    \n",
    "    def fit(self, docs):\n",
    "        self.preprocess_docs = [Tokenizer(doc, RemoveStopWords=self.RemoveStopWords).tokens for doc in docs]\n",
    "        \n",
    "        for doc in self.preprocess_docs:\n",
    "            for word in doc:\n",
    "                self.vocabulary_[word] = self.vocabulary_.get(word, 0) + 1\n",
    "        \n",
    "        self.dictionary_ = {w:i for i,w in enumerate(self.vocabulary_)}\n",
    "        self.total_words = len(self.vocabulary_)\n",
    "        self.total_docs = len(docs)\n",
    "        return self\n",
    "    \n",
    "    def get_CountVectorizer(self):\n",
    "        for doc in self.preprocess_docs:\n",
    "            cv = [0]*self.total_words\n",
    "            for w in doc:\n",
    "                j = self.dictionary_[w]\n",
    "                cv[j] += 1\n",
    "            self.cv.append(cv)\n",
    "        return self.cv\n",
    "        \n",
    "    def get_TFIDF(self):\n",
    "        self.cv = self.get_CountVectorizer()\n",
    "        for i in range(self.total_docs):\n",
    "            tfidf = [0]*self.total_words\n",
    "            for word in self.preprocess_docs[i]:\n",
    "                j = self.dictionary_[word]\n",
    "                tfidf[j] += self.cv[i][j]/self.vocabulary_[word]\n",
    "            self.tfidf.append(tfidf)\n",
    "        return self.tfidf\n",
    "\n",
    "    \n",
    "class TextSimilarity:\n",
    "    def __init__(self, docs, RemoveStopWords=True):\n",
    "        self.docs = docs\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "    \n",
    "    def get_cosin_similarity(self, v1, v2):\n",
    "        # cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "        v1_dot_v2 = 0\n",
    "        norm_v1 = 0\n",
    "        norm_v2 = 0\n",
    "        \n",
    "        for i in range(len(v1)):\n",
    "            v1_dot_v2 += v1[i]*v2[i]\n",
    "            norm_v1 += v1[i]**2\n",
    "            norm_v2 += v2[i]**2\n",
    "        \n",
    "        return v1_dot_v2/(norm_v1*norm_v2)**0.5\n",
    "        \n",
    "    def get_pairwise_similarity(self, v1, v2):\n",
    "        sim = 0\n",
    "        for i in range(len(v1)):\n",
    "            sim += v1[i]*v2[i]\n",
    "        return sim\n",
    "    \n",
    "    def get_similarities(self):\n",
    "        countVectorizer = Vectorizer(RemoveStopWords=self.RemoveStopWords).fit(self.docs)\n",
    "        count_vec = countVectorizer.get_CountVectorizer()\n",
    "        tfidf_vec = countVectorizer.get_TFIDF()\n",
    "        \n",
    "        cosin_sims = {}\n",
    "        pairwise_sims = {}\n",
    "        for i in range(countVectorizer.total_docs):\n",
    "            for j in range(i+1, countVectorizer.total_docs):\n",
    "                cosin_sims[f'Text{i}-Text{j}'] = self.get_cosin_similarity(count_vec[i], count_vec[j])\n",
    "                pairwise_sims[f'Text{i}-Text{j}'] = self.get_pairwise_similarity(tfidf_vec[i], tfidf_vec[j])\n",
    "                \n",
    "        \n",
    "        return cosin_sims, pairwise_sims\n",
    "    \n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"\"\"We are always looking for opportunities for you to earn more points, \n",
    "which is why we also give you a selection of Special Offers. These Special Offers are \n",
    "opportunities to earn bonus points on top of the regular points you earn every time you purchase a \n",
    "participating brand. No need to pre-select these offers, we'll give you the points whether or not \n",
    "you knew about the offer. We just think it is easier that way.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. \n",
    "If you have any eligible brands on your receipt, you will get points based on the total cost of the products. \n",
    "You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out \n",
    "and we will find the savings for you.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text1 = \"\"\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. \n",
    "If you have any participating brands on your receipt, you'll get points based on the cost of the products. \n",
    "You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop \n",
    "and we'll find the savings for you.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "docs = [text1, text2, text3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim, pair_sim = TextSimilarity(docs, RemoveStopWords=True).get_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text0-Text1': 6.401111111111111,\n",
       " 'Text0-Text2': 1.8322222222222222,\n",
       " 'Text1-Text2': 1.5822222222222222}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\"I'd like an apple\", \n",
    "          \"An apple a day keeps the doctor away\", \n",
    "          \"Never compare an apple to an orange\", \n",
    "          \"I prefer scikit-learn to Orange\", \n",
    "          \"The scikit-learn docs are Orange and Blue\"]\n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "tfidf = vect.fit_transform(corpus)\n",
    "pairwise_similarity = tfidf*tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.830880748357988\n",
      "  (0, 0)\t0.5564505207186616\n",
      "  (1, 0)\t0.31752680284846835\n",
      "  (1, 4)\t0.4741246485558491\n",
      "  (1, 7)\t0.4741246485558491\n",
      "  (1, 6)\t0.4741246485558491\n",
      "  (1, 1)\t0.4741246485558491\n",
      "  (2, 0)\t0.48624041659157047\n",
      "  (2, 3)\t0.7260444301457811\n",
      "  (2, 10)\t0.48624041659157047\n",
      "  (3, 10)\t0.40382592962643526\n",
      "  (3, 11)\t0.6029847724484662\n",
      "  (3, 12)\t0.4864843177105593\n",
      "  (3, 8)\t0.4864843177105593\n",
      "  (4, 10)\t0.345821664219199\n",
      "  (4, 12)\t0.4166072657167828\n",
      "  (4, 8)\t0.4166072657167828\n",
      "  (4, 5)\t0.5163739676148649\n",
      "  (4, 2)\t0.5163739676148649\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.27056873300683837\n",
      "  (0, 1)\t0.17668795478716204\n",
      "  (0, 0)\t0.9999999999999998\n",
      "  (1, 2)\t0.1543943648960287\n",
      "  (1, 1)\t0.9999999999999999\n",
      "  (1, 0)\t0.17668795478716204\n",
      "  (2, 4)\t0.16815247007633352\n",
      "  (2, 3)\t0.1963564882520361\n",
      "  (2, 2)\t1.0\n",
      "  (2, 1)\t0.1543943648960287\n",
      "  (2, 0)\t0.27056873300683837\n",
      "  (3, 4)\t0.5449975578692605\n",
      "  (3, 3)\t0.9999999999999999\n",
      "  (3, 2)\t0.1963564882520361\n",
      "  (4, 4)\t0.9999999999999996\n",
      "  (4, 3)\t0.5449975578692605\n",
      "  (4, 2)\t0.16815247007633352\n"
     ]
    }
   ],
   "source": [
    "print(pairwise_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = TextSimilarity(docs, use_tfidf=False, RemoveStopWords=False).get_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text, RemoveStopWords=True):\n",
    "        self.text = text\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "        self.punctuations = ['.', '!', '?', '\\'', '\\\"',\n",
    "                             ',', ':', ';', '(', ')', '[', ']', '<', '>', '\\n']\n",
    "        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "                           'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                           'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "                           'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be',\n",
    "                           'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                           'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "                           'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                           'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                           'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                           'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "                           'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',\n",
    "                           'now', 'm', 're', 'would', 'd', 'll']\n",
    "        self.tokens = self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        text = self.text.lower()\n",
    "        text = text.replace('-', '')\n",
    "        for p in self.punctuations:\n",
    "            text = text.replace(p, ' ')\n",
    "\n",
    "        words = [w for w in text.split(' ') if w != '']\n",
    "        self.total_words = len(words)\n",
    "        self.stopWords = len([w for w in words if w in self.stop_words])\n",
    "        \n",
    "        if self.RemoveStopWords:\n",
    "            tokens = [word for word in words if word not in self.stop_words]\n",
    "        else:\n",
    "            tokens = words\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, RemoveStopWords=True):\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "        self.vocabulary_ = {}\n",
    "        self.dictionary_ = {}\n",
    "        self.cv = []\n",
    "        self.tfidf = []\n",
    "\n",
    "    def fit(self, docs):\n",
    "        self.preprocess_docs = [\n",
    "            Tokenizer(doc, RemoveStopWords=self.RemoveStopWords).tokens for doc in docs]\n",
    "\n",
    "        for doc in self.preprocess_docs:\n",
    "            for word in doc:\n",
    "                self.vocabulary_[word] = self.vocabulary_.get(word, 0) + 1\n",
    "\n",
    "        self.dictionary_ = {w: i for i, w in enumerate(self.vocabulary_)}\n",
    "        self.total_words = len(self.vocabulary_)\n",
    "        self.total_docs = len(docs)\n",
    "        return self\n",
    "\n",
    "    def get_CountVectorizer(self):\n",
    "        for doc in self.preprocess_docs:\n",
    "            cv = [0]*self.total_words\n",
    "            for w in doc:\n",
    "                j = self.dictionary_[w]\n",
    "                cv[j] += 1\n",
    "            self.cv.append(cv)\n",
    "        return self.cv\n",
    "\n",
    "    def get_TFIDF(self):\n",
    "        self.cv = self.get_CountVectorizer()\n",
    "        for i in range(self.total_docs):\n",
    "            tfidf = [0]*self.total_words\n",
    "            for word in self.preprocess_docs[i]:\n",
    "                j = self.dictionary_[word]\n",
    "                tfidf[j] += self.cv[i][j]/self.vocabulary_[word]\n",
    "            self.tfidf.append(tfidf)\n",
    "        return self.tfidf\n",
    "\n",
    "\n",
    "class TextSimilarity:\n",
    "    def __init__(self, docs, RemoveStopWords=True, use_tfidf=False):\n",
    "        self.docs = docs\n",
    "        self.RemoveStopWords = RemoveStopWords\n",
    "        self.use_tfidf = use_tfidf\n",
    "\n",
    "    def get_cosin_similarity(self, v1, v2):\n",
    "        # cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "        v1_dot_v2 = 0\n",
    "        norm_v1 = 0\n",
    "        norm_v2 = 0\n",
    "\n",
    "        for i in range(len(v1)):\n",
    "            v1_dot_v2 += v1[i]*v2[i]\n",
    "            norm_v1 += v1[i]**2\n",
    "            norm_v2 += v2[i]**2\n",
    "\n",
    "        return v1_dot_v2/(norm_v1*norm_v2)**0.5\n",
    "\n",
    "    def get_similarities(self):\n",
    "        countVectorizer = Vectorizer(\n",
    "            RemoveStopWords=self.RemoveStopWords).fit(self.docs)\n",
    "        if self.use_tfidf:\n",
    "            vec = countVectorizer.get_TFIDF()\n",
    "        else:\n",
    "            vec = countVectorizer.get_CountVectorizer()\n",
    "\n",
    "        similarities = {}\n",
    "\n",
    "        for i in range(countVectorizer.total_docs):\n",
    "            for j in range(i+1, countVectorizer.total_docs):\n",
    "                similarities[f'Text{i}-Text{j}'] = self.get_cosin_similarity(\n",
    "                    vec[i], vec[j])\n",
    "        return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text0-Text1': 0.8169217288768313}\n"
     ]
    }
   ],
   "source": [
    "docs = [text1, text2]\n",
    "sim = TextSimilarity(docs, RemoveStopWords=True,\n",
    "                     use_tfidf=False).get_similarities()\n",
    "print(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
